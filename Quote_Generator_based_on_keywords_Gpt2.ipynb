{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In this notebook we will tune the GPT2 to generate anime character dialog lines based on the keywords.\n",
        "# Basically it is a conditional text generation problem. \n",
        "# Already trained transformer based text generation algorithms generates text based on the casual language masking or masked language masking. So i am going to tune a gpt architecture which will generate new text based on keywords which we will insert in the input and the text will be generated by catering the keywords."
      ],
      "metadata": {
        "id": "K8QtCLXo_nwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing huggingface transformers"
      ],
      "metadata": {
        "id": "FKcM5R-2AhVx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noFpdde7Q1bw",
        "outputId": "58182a04-9215-4af1-93be-0db3005aae66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 24.5 ms, sys: 8.52 ms, total: 33.1 ms\n",
            "Wall time: 3.63 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%%capture\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Important Libraries"
      ],
      "metadata": {
        "id": "smbGxm6hAlFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import zipfile\n",
        "import random\n",
        "import time\n",
        "import csv\n",
        "import datetime\n",
        "from itertools import compress\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n",
        "                         AdamW, get_linear_schedule_with_warmup, \\\n",
        "                         TrainingArguments, BeamScorer, Trainer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, \\\n",
        "                             RandomSampler, SequentialSampler\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy8TuywSROgu",
        "outputId": "0745cfa4-884c-426a-f242-ba126e2d55c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 1.12.1+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters Initialization"
      ],
      "metadata": {
        "id": "O3Q4Q4aSAsdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG           = False\n",
        "\n",
        "USE_APEX        = True\n",
        "APEX_OPT_LEVEL  = 'O1'\n",
        "\n",
        "# You can use any version of the gpt according to your computation powers\n",
        "MODEL           = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n",
        "\n",
        "# if you want to fine tune only some layers of \n",
        "# transformers you can change the number of layers to be \n",
        "# tuned in the given variable\n",
        "#UNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n",
        "\n",
        "# These are the special tokens which will help\n",
        "# in seperating the keywords during the Dataset loading \n",
        "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
        "                    \"eos_token\": \"<|EOS|>\",\n",
        "                    \"unk_token\": \"<|UNK|>\",                    \n",
        "                    \"pad_token\": \"<|PAD|>\",\n",
        "                    \"sep_token\": \"<|SEP|>\"}\n",
        "\n",
        "\n",
        "MAXLEN          = 768  #{768, 1024, 1280, 1600}\n",
        "\n",
        "# 80% of the data will be used as Training data\n",
        "# 20% will be used as test data\n",
        "TRAIN_SIZE      = 0.8\n",
        "\n",
        "if USE_APEX:\n",
        "    TRAIN_BATCHSIZE = 4\n",
        "    BATCH_UPDATE    = 16\n",
        "else:\n",
        "    TRAIN_BATCHSIZE = 2\n",
        "    BATCH_UPDATE    = 32\n",
        "\n",
        "EPOCHS          = 4\n",
        "LR              = 5e-4 # Learning rate\n",
        "EPS             = 1e-8\n",
        "WARMUP_STEPS    = 1e2\n",
        "\n",
        "SEED            = 2020"
      ],
      "metadata": {
        "id": "cI8U1EvLRREt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function will seed every thing so that the results will become \n",
        "# constant accross different experiments\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ],
      "metadata": {
        "id": "I6AWC3C9RTSt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prepration"
      ],
      "metadata": {
        "id": "VzRNWTlJRaq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Quotes_with_keyword.csv',usecols=['Quote','Keyword'])"
      ],
      "metadata": {
        "id": "jJzVgOqaRUv0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "f7a268vMRgss",
        "outputId": "4d87acf6-e823-4491-da87-0ba3bbe34f75"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Quote      Keyword\n",
              "0  In the end the shape and form don't matter at ...       matter\n",
              "1  I'm still a man too, I wanted to look calm and...         love\n",
              "2  Clausewitz, he pointed out that no matter how ...     armchair\n",
              "3  Because of the existence of love - sacrifice i...  comprehends\n",
              "4  Courage is a word of justice. It means the qua...       excuse"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f86ec30d-cf0e-43f2-b697-abf7daa64851\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Quote</th>\n",
              "      <th>Keyword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In the end the shape and form don't matter at ...</td>\n",
              "      <td>matter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'm still a man too, I wanted to look calm and...</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Clausewitz, he pointed out that no matter how ...</td>\n",
              "      <td>armchair</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Because of the existence of love - sacrifice i...</td>\n",
              "      <td>comprehends</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Courage is a word of justice. It means the qua...</td>\n",
              "      <td>excuse</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f86ec30d-cf0e-43f2-b697-abf7daa64851')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f86ec30d-cf0e-43f2-b697-abf7daa64851 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f86ec30d-cf0e-43f2-b697-abf7daa64851');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a dictionary of the data"
      ],
      "metadata": {
        "id": "Ffvc3Pk0BtGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = dict()\n",
        "for index, row in df.iterrows():\n",
        "  # data_list will look like\n",
        "  # {index, [Quote,[keyword]]}\n",
        "  # casting keyword in the list because if in future \n",
        "  # if we have to embedd more keywords then we will use the same code\n",
        "  # currently only we are giving just on keyword \n",
        "  data_list[index] = [row['Quote'],[row['Keyword']]] "
      ],
      "metadata": {
        "id": "zUNyjcOyRhbE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function will split the training data and test data according to the ratio"
      ],
      "metadata": {
        "id": "RbsDtpESBzKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data, S=TRAIN_SIZE):\n",
        "    # Shuffle ids\n",
        "    ids = list(data.keys())\n",
        "    random.shuffle(ids)\n",
        "\n",
        "    # Split into training and validation sets    \n",
        "    train_size = int(S * len(data))\n",
        "\n",
        "    train_ids = ids[:train_size]\n",
        "    val_ids = ids[train_size:]\n",
        "\n",
        "    train_data = dict()\n",
        "    for id in train_ids:\n",
        "        train_data[id] = data[id]\n",
        "\n",
        "    val_data = dict()\n",
        "    for id in val_ids:\n",
        "        val_data[id] = data[id]\n",
        "\n",
        "    return train_data, val_data"
      ],
      "metadata": {
        "id": "Ybz6bR4OSYnN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Dataset Class"
      ],
      "metadata": {
        "id": "SA-a5zXiB6ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset class will help in making a dataset which will embedd keywords in to the prompts so that while training the architecture will keep the keywords in computation for learning the sequences"
      ],
      "metadata": {
        "id": "_QHWuvLZB8Db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, randomize=True):\n",
        "        text, keywords = [], []\n",
        "        for k, v in data.items():\n",
        "            text.append(v[0])\n",
        "            keywords.append(v[1])\n",
        "\n",
        "        self.randomize = randomize\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text      = text\n",
        "        self.keywords  = keywords  \n",
        "\n",
        "    #---------------------------------------------#\n",
        "\n",
        "    @staticmethod\n",
        "    def join_keywords(keywords, randomize=True):\n",
        "        N = len(keywords)\n",
        "\n",
        "        #random sampling and shuffle\n",
        "        if randomize: \n",
        "            M = random.choice(range(N+1))\n",
        "            keywords = keywords[:M]\n",
        "            random.shuffle(keywords)\n",
        "\n",
        "        return ','.join(keywords)\n",
        "\n",
        "    #---------------------------------------------#\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    #---------------------------------------------#\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        keywords = self.keywords[i].copy()\n",
        "        kw = self.join_keywords(keywords, self.randomize) \n",
        "        \n",
        "        # input prompt will now contain keyword and text both\n",
        "        input = SPECIAL_TOKENS['bos_token'] + kw + SPECIAL_TOKENS['sep_token'] + \\\n",
        "                self.text[i] + SPECIAL_TOKENS['eos_token']\n",
        "\n",
        "        # tokenization of the prompt\n",
        "        encodings_dict = tokenizer(input,                                   \n",
        "                                   truncation=True, \n",
        "                                   max_length=MAXLEN, \n",
        "                                   padding=\"max_length\")   \n",
        "        \n",
        "        input_ids = encodings_dict['input_ids']\n",
        "        attention_mask = encodings_dict['attention_mask']\n",
        "        \n",
        "        # returning the tokenized data\n",
        "        return {'label': torch.tensor(input_ids),\n",
        "                'input_ids': torch.tensor(input_ids), \n",
        "                'attention_mask': torch.tensor(attention_mask)}"
      ],
      "metadata": {
        "id": "SCAFcbw-S6K9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer and Model initialization"
      ],
      "metadata": {
        "id": "ebNIvX9bC4kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get tokenizer\n",
        "def get_tokenier(special_tokens=None):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n",
        "\n",
        "    if special_tokens:\n",
        "        tokenizer.add_special_tokens(special_tokens)\n",
        "        print(\"Special tokens added\")\n",
        "    return tokenizer\n",
        "\n",
        "# Function to get Model\n",
        "def get_model(tokenizer, special_tokens=None, load_model_path=None):\n",
        "\n",
        "    #GPT2LMHeadModel\n",
        "    if special_tokens:\n",
        "      # changing the config to induce special token\n",
        "        config = AutoConfig.from_pretrained(MODEL, \n",
        "                                            bos_token_id=tokenizer.bos_token_id,\n",
        "                                            eos_token_id=tokenizer.eos_token_id,\n",
        "                                            sep_token_id=tokenizer.sep_token_id,\n",
        "                                            pad_token_id=tokenizer.pad_token_id,\n",
        "                                            output_hidden_states=False)\n",
        "    else: \n",
        "        config = AutoConfig.from_pretrained(MODEL,                                     \n",
        "                                            pad_token_id=tokenizer.eos_token_id,\n",
        "                                            output_hidden_states=False)    \n",
        "\n",
        "    #----------------------------------------------------------------#\n",
        "    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config)\n",
        "\n",
        "    if special_tokens:\n",
        "        #Special tokens added, model needs to be resized accordingly\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if load_model_path:\n",
        "        model.load_state_dict(torch.load(load_model_path))\n",
        "\n",
        "    model.cuda()\n",
        "    return model"
      ],
      "metadata": {
        "id": "3VZWX48AS8Yc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
        "model = get_model(tokenizer, \n",
        "                  special_tokens=SPECIAL_TOKENS,\n",
        "                #   load_model_path='pytorch_model.bin'\n",
        "                 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WaJ0PKcTFwk",
        "outputId": "d355ce07-bd14-421e-c3e3-595e2bdef536"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special tokens added\n",
            "CPU times: user 4.55 s, sys: 1.67 s, total: 6.22 s\n",
            "Wall time: 7.69 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### You can uncomment this block to only fine tune layers of your own choice"
      ],
      "metadata": {
        "id": "oevzGk4PDDgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # - Freeze selective layers:\n",
        "# # - Freeze all layers except last n:\n",
        "# for parameter in model.parameters():\n",
        "#     parameter.requires_grad = False\n",
        "\n",
        "# for i, m in enumerate(model.transformer.h):        \n",
        "#     #Only un-freeze the last n transformer blocks\n",
        "#     if i+1 > 12 - UNFREEZE_LAST_N:\n",
        "#         for parameter in m.parameters():\n",
        "#             parameter.requires_grad = True \n",
        "\n",
        "# for parameter in model.transformer.ln_f.parameters():        \n",
        "#     parameter.requires_grad = True\n",
        "\n",
        "# for parameter in model.lm_head.parameters():        \n",
        "#     parameter.requires_grad = True"
      ],
      "metadata": {
        "id": "2kFFlj_zTHZU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the dataset and making training and validation/test dataset"
      ],
      "metadata": {
        "id": "S66gYRSMDIhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_list\n",
        "train_data, val_data = split_data(data)\n",
        "\n",
        "train_dataset = myDataset(train_data, tokenizer)\n",
        "val_dataset = myDataset(val_data, tokenizer, randomize=False)"
      ],
      "metadata": {
        "id": "AITpNaerSzMk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the training arguments and hyperparameters"
      ],
      "metadata": {
        "id": "4hdt7ilwDSY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/\",\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=TRAIN_BATCHSIZE,\n",
        "    per_device_eval_batch_size=TRAIN_BATCHSIZE,\n",
        "    gradient_accumulation_steps=BATCH_UPDATE,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    fp16_opt_level=APEX_OPT_LEVEL,\n",
        "    warmup_steps=WARMUP_STEPS,    \n",
        "    learning_rate=LR,\n",
        "    adam_epsilon=EPS,\n",
        "    weight_decay=0.01,        \n",
        "    save_total_limit=1     \n",
        ")\n",
        "\n",
        "#---------------------------------------------------#\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,    \n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# #---------------------------------------------------#\n",
        "trainer.train() # Starts the traning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        },
        "id": "pyIUW5FmT24c",
        "outputId": "fb7f51b1-cb2f-42a9-df2b-5dd728272db6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 6684\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 416\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='416' max='416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [416/416 51:53, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.154708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.150050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.150614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.153365</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1672\n",
            "  Batch size = 4\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1672\n",
            "  Batch size = 4\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1672\n",
            "  Batch size = 4\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1672\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 51min 58s, sys: 6.09 s, total: 52min 4s\n",
            "Wall time: 52min\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=416, training_loss=0.28583033268268293, metrics={'train_runtime': 3120.5837, 'train_samples_per_second': 8.568, 'train_steps_per_second': 0.133, 'total_flos': 1.0467881385984e+16, 'train_loss': 0.28583033268268293, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "UJXtqhNKDa4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('/content/Trained_Model1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gJ2xnmvUE4c",
        "outputId": "e9dddf8f-6fc5-4b52-bb5a-6efece0986ec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/Trained_Model1\n",
            "Configuration saved in /content/Trained_Model1/config.json\n",
            "Model weights saved in /content/Trained_Model1/pytorch_model.bin\n",
            "tokenizer config file saved in /content/Trained_Model1/tokenizer_config.json\n",
            "Special tokens file saved in /content/Trained_Model1/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the tokenizer and model for prediction"
      ],
      "metadata": {
        "id": "e2es4f4qDcc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
        "model = get_model(tokenizer, \n",
        "                  special_tokens=SPECIAL_TOKENS,\n",
        "                  load_model_path='/content/Trained_Model/pytorch_model.bin')"
      ],
      "metadata": {
        "id": "sK8lVbgXcUKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = ['evil']\n",
        "kw = myDataset.join_keywords(keywords, randomize=False)\n",
        "\n",
        "prompt = SPECIAL_TOKENS['bos_token'] + kw + SPECIAL_TOKENS['sep_token']\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "device = torch.device(\"cuda\")\n",
        "generated = generated.to(device)\n",
        "\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "aLstoL5-blms"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Beam-search text generation:\n",
        "sample_outputs = model.generate(generated, \n",
        "                                do_sample=True,   \n",
        "                                max_length=MAXLEN,                                                      \n",
        "                                num_beams=5,\n",
        "                                repetition_penalty=5.0,\n",
        "                                early_stopping=True,      \n",
        "                                num_return_sequences=10\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
        "    a = len(','.join(keywords))    \n",
        "    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vICClUtcEF8",
        "outputId": "6a6e1ffc-2da1-4a8d-d5ce-f6d94df45d11"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: There is no such thing as evil. It's just that you can't live without it.\n",
            "\n",
            "\n",
            "2: If you're looking for a good place to live, this is it. It's the only place I can think of that makes me happy.\n",
            "\n",
            "\n",
            "3: There's nothing wrong with that. I'm just saying, if you don't like it, then go ahead and get rid of it.\n",
            "\n",
            "\n",
            "4: It's not just me. It's all of you, too.\n",
            "\n",
            "\n",
            "5: I'm not a bad person, I just don't want to be evil.\n",
            "\n",
            "\n",
            "6: If you want to know what the hell is going on in this world, I'll show you.\n",
            "\n",
            "\n",
            "7: I don't want to be the one who says, 'Hey, I'm not going to let you kill me. You're just a coward.'\n",
            "\n",
            "\n",
            "8: There are many things that can be done to help save the world.\n",
            "\n",
            "\n",
            "9: If you don't know what the hell I'm talking about, just take a look at me.\n",
            "\n",
            "\n",
            "10: Humans are the only ones who can truly understand what it means to be human.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/Trained_Model1.zip /content/Trained_Model1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gJAWwQ-gwvr",
        "outputId": "3ce13aa0-e904-48a9-d815-812038030902"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/Trained_Model1/ (stored 0%)\n",
            "  adding: content/Trained_Model1/merges.txt (deflated 53%)\n",
            "  adding: content/Trained_Model1/special_tokens_map.json (deflated 48%)\n",
            "  adding: content/Trained_Model1/config.json (deflated 52%)\n",
            "  adding: content/Trained_Model1/tokenizer.json (deflated 72%)\n",
            "  adding: content/Trained_Model1/tokenizer_config.json (deflated 41%)\n",
            "  adding: content/Trained_Model1/pytorch_model.bin (deflated 9%)\n",
            "  adding: content/Trained_Model1/training_args.bin (deflated 48%)\n",
            "  adding: content/Trained_Model1/vocab.json (deflated 59%)\n",
            "  adding: content/Trained_Model1/added_tokens.json (deflated 46%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LwC3biQGwefk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}